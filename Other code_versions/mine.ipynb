{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "48ae22b9",
      "metadata": {
        "id": "48ae22b9"
      },
      "source": [
        "### Complete PET-to-CT Translation Pipeline\n",
        " **Architecture**: ResNet-34 Encoder + ViT Bottleneck + CNN Decoder  \n",
        " **Features**:\n",
        " - TCIA API download\n",
        " - NPY/PNG preprocessing (7GB storage)\n",
        " - Mixed precision training\n",
        " - Multi-scale SSIM loss\n",
        " - Model checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc975934",
      "metadata": {
        "id": "bc975934"
      },
      "source": [
        "### 0. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "49730fcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49730fcb",
        "outputId": "8f646d08-1b8e-4536-ed19-f6a536ebafc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install pydicom numpy pillow tqdm requests torch torchvision pytorch-msssim einops kaggle --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "L6oA9De2ZnK9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6oA9De2ZnK9",
        "outputId": "e76392a6-6d76-4a92-8582-f9b03635976c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ],
      "source": [
        "%pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a1834e3d",
      "metadata": {
        "id": "a1834e3d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool\n",
        "from pytorch_msssim import MS_SSIM\n",
        "from einops import rearrange\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import optuna\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0358ae7c",
      "metadata": {
        "id": "0358ae7c"
      },
      "source": [
        "#### 1. Download QIN-Breast from TCIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7f790ac7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "7f790ac7",
        "outputId": "9e69ce94-249c-474f-cf06-b09a21b24766"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5b7a10922512>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Input directories for your original DICOM files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpet_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/PIX/PET'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Input directories for your original DICOM files\n",
        "pet_dir = '/content/drive/MyDrive/PIX/PET'\n",
        "ct_dir  = '/content/drive/MyDrive/PIX/CT'\n",
        "\n",
        "# Output directories for processed .npy files\n",
        "processed_pet_dir = '/content/drive/MyDrive/PIX/processed/PET'\n",
        "processed_ct_dir  = '/content/drive/MyDrive/PIX/processed/CT'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "171f0a97",
      "metadata": {
        "id": "171f0a97"
      },
      "source": [
        "#### 2. Preprocess to NPY/PNG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "604fa964",
      "metadata": {
        "id": "604fa964"
      },
      "outputs": [],
      "source": [
        "def process_dicom_file(args):\n",
        "    \"\"\"\n",
        "    Convert a DICOM (.dcm) file to a NumPy (.npy) file.\n",
        "\n",
        "    Args:\n",
        "        args (tuple): Contains:\n",
        "                      - dicom_file (str): Full path to the DICOM file.\n",
        "                      - output_dir (str): Directory where the .npy file will be saved.\n",
        "    \"\"\"\n",
        "    dicom_file, output_dir = args\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        ds = pydicom.dcmread(dicom_file)\n",
        "        img_array = ds.pixel_array  # Extract the image as a NumPy array\n",
        "\n",
        "        # Replace the .dcm extension with .npy for the output file\n",
        "        base_name = os.path.basename(dicom_file)\n",
        "        output_filename = os.path.splitext(base_name)[0] + '.npy'\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        # Save the NumPy array\n",
        "        np.save(output_path, img_array)\n",
        "        print(f\"Converted '{dicom_file}' to '{output_path}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {dicom_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c19328e6",
      "metadata": {
        "id": "c19328e6"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(pet_dir, ct_dir, processed_pet_dir, processed_ct_dir):\n",
        "    \"\"\"\n",
        "    Process all DICOM files in PET and CT directories and convert them to .npy files.\n",
        "    \"\"\"\n",
        "    # Process PET files\n",
        "    pet_files = sorted([os.path.join(pet_dir, f) for f in os.listdir(pet_dir) if f.endswith('.dcm')])\n",
        "    print(f\"Found {len(pet_files)} PET files.\")\n",
        "    for dicom_file in pet_files:\n",
        "        process_dicom_file((dicom_file, processed_pet_dir))\n",
        "\n",
        "    # Process CT files\n",
        "    ct_files = sorted([os.path.join(ct_dir, f) for f in os.listdir(ct_dir) if f.endswith('.dcm')])\n",
        "    print(f\"Found {len(ct_files)} CT files.\")\n",
        "    for dicom_file in ct_files:\n",
        "        process_dicom_file((dicom_file, processed_ct_dir))\n",
        "\n",
        "    print(\"Preprocessing complete.\")\n",
        "\n",
        "# Run preprocessing (if you haven't done it already)\n",
        "preprocess_dataset(pet_dir, ct_dir, processed_pet_dir, processed_ct_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f3ca235",
      "metadata": {
        "id": "5f3ca235"
      },
      "source": [
        "#### Step 2. Split the Processed Dataset\n",
        "Since your processed files now exist as .npy files in separate folders for PET and CT—with matching filenames—we can list the PET folder (or CT, as they should be identical) and then randomly split that list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ebd6819",
      "metadata": {
        "id": "3ebd6819"
      },
      "outputs": [],
      "source": [
        "def get_file_splits(processed_pet_dir, test_size=0.1, val_size=0.1):\n",
        "    \"\"\"\n",
        "    Split the dataset based on the .npy files in the processed_pet_dir.\n",
        "    Assumes that processed_pet_dir and processed_ct_dir contain matching filenames.\n",
        "\n",
        "    This function returns three lists: train, validation, and test filenames.\n",
        "    \"\"\"\n",
        "    all_files = sorted([f for f in os.listdir(processed_pet_dir) if f.endswith('.npy')])\n",
        "    print(\"Total number of processed paired images:\", len(all_files))\n",
        "\n",
        "    # First, split into train and temporary set (val + test)\n",
        "    train_files, temp_files = train_test_split(all_files, test_size=(test_size + val_size), random_state=42)\n",
        "\n",
        "    # Then, split the temporary set into validation and test set equally, based on provided ratios.\n",
        "    val_ratio = val_size / (test_size + val_size)\n",
        "    val_files, test_files = train_test_split(temp_files, test_size=(1 - val_ratio), random_state=42)\n",
        "\n",
        "    print(\"Train samples:\", len(train_files))\n",
        "    print(\"Validation samples:\", len(val_files))\n",
        "    print(\"Test samples:\", len(test_files))\n",
        "\n",
        "    return train_files, val_files, test_files\n",
        "\n",
        "# Get the file splits\n",
        "#train_files, val_files, test_files = get_file_splits(processed_pet_dir, test_size=0.1, val_size=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a007b5b5",
      "metadata": {
        "id": "a007b5b5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class QinBreastDataset(Dataset):\n",
        "    def __init__(self, filenames, processed_pet_dir, processed_ct_dir, transform=None):\n",
        "        \"\"\"\n",
        "        filenames: list of filenames (e.g., \"1-01.npy\") existing in both directories.\n",
        "        processed_pet_dir: Directory where processed PET .npy files are stored.\n",
        "        processed_ct_dir: Directory where processed CT .npy files are stored.\n",
        "        transform: Optional transform (e.g., normalization, conversion to tensor) to apply.\n",
        "        \"\"\"\n",
        "        self.filenames = filenames\n",
        "        self.pet_dir = processed_pet_dir\n",
        "        self.ct_dir = processed_ct_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.filenames[idx]\n",
        "        pet_path = os.path.join(self.pet_dir, filename)\n",
        "        ct_path  = os.path.join(self.ct_dir, filename)\n",
        "\n",
        "        pet_img = np.load(pet_path)\n",
        "        ct_img  = np.load(ct_path)\n",
        "\n",
        "        if self.transform:\n",
        "            pet_img = self.transform(pet_img)\n",
        "            ct_img = self.transform(ct_img)\n",
        "\n",
        "        return pet_img, ct_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdc5e25",
      "metadata": {
        "id": "3fdc5e25"
      },
      "source": [
        "#### 4. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf3ffd3",
      "metadata": {
        "id": "0cf3ffd3"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# ======================\n",
        "\n",
        "class ViTBlock(nn.Module):\n",
        "    def __init__(self, dim=512, heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(dim, heads, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, dim*4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),  #GEL nad Dropout for better stability\n",
        "            nn.Linear(dim*4, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attention(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        mlp_out = self.mlp(x)\n",
        "        return self.norm2(x + mlp_out)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder (ResNet-34)\n",
        "        #resnet = models.resnet34(pretrained=True)\n",
        "        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            *list(resnet.children())[1:-2]  # Remove original fc layer\n",
        "        )\n",
        "\n",
        "        # ViT Bottleneck\n",
        "        self.vit = nn.Sequential(\n",
        "            ViTBlock(dim=512),\n",
        "            #ViTBlock(dim=512),\n",
        "           # ViTBlock(dim=512)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        b, c, h, w = x.shape\n",
        "        x = rearrange(x, 'b c h w -> (h w) b c')\n",
        "        x = self.vit(x)\n",
        "        x = rearrange(x, '(h w) b c -> b c h w', h=h, w=w)\n",
        "        #return self.decoder(x)\n",
        "        return self.decoder(x.to(device))  # # Move output tensor back to GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfd988e2",
      "metadata": {
        "id": "cfd988e2"
      },
      "source": [
        "Multi-Scale Discriminator is designed to assess images at different resolutions, improving adversarial learning stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8311261f",
      "metadata": {
        "id": "8311261f"
      },
      "outputs": [],
      "source": [
        "class MultiScaleDiscriminator(nn.Module):\n",
        "    def __init__(self, input_channels=1):\n",
        "        super().__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            self._make_discriminator(input_channels, 64),\n",
        "            self._make_discriminator(input_channels, 32),\n",
        "            self._make_discriminator(input_channels, 16)\n",
        "        ])\n",
        "\n",
        "    def _make_discriminator(self, in_ch, base_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(in_ch, base_ch, 4, 2, 1)),  #Improves training stability by constraining weight norms.\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(base_ch, base_ch*2, 4, 2, 1)),\n",
        "            nn.InstanceNorm2d(base_ch*2),         #Helps normalize features, preventing vanishing or exploding gradients.\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(base_ch*2, 1, 4, 1, 1)),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        x = x.to(device)  # Ensure tensor is on GPU\n",
        "        for disc in self.discriminators:\n",
        "            outputs.append(disc(x))\n",
        "            #x = nn.functional.interpolate(x, scale_factor=0.5, mode='bilinear')\n",
        "            x = nn.functional.interpolate(x, scale_factor=0.5, mode='nearest')\n",
        "         #return torch.cat(outputs, dim=1)\n",
        "        return torch.cat(outputs, dim=1).to(device)  # Keep output on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6c0f8c",
      "metadata": {
        "id": "fb6c0f8c"
      },
      "source": [
        "####  5. Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5845ea47",
      "metadata": {
        "id": "5845ea47"
      },
      "outputs": [],
      "source": [
        "#from torchvision.models import vgg19\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg19, VGG19_Weights\n",
        "\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        # Load a pre-trained VGG19 model\n",
        "        vgg = vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        # Break the network into slices corresponding to different layers' outputs.\n",
        "        self.slice1 = nn.Sequential(*[vgg[x] for x in range(2)])\n",
        "        self.slice2 = nn.Sequential(*[vgg[x] for x in range(2, 7)])\n",
        "        self.slice3 = nn.Sequential(*[vgg[x] for x in range(7, 12)])\n",
        "        self.slice4 = nn.Sequential(*[vgg[x] for x in range(12, 21)])\n",
        "        self.slice5 = nn.Sequential(*[vgg[x] for x in range(21, 30)])\n",
        "\n",
        "        # Freeze the VGG parameters if not training them.\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Compute feature maps at various depths\n",
        "        loss = 0\n",
        "        x1, y1 = self.slice1(x), self.slice1(y)\n",
        "        loss += F.l1_loss(x1, y1)\n",
        "\n",
        "        x2, y2 = self.slice2(x), self.slice2(y)\n",
        "        loss += F.l1_loss(x2, y2)\n",
        "\n",
        "        x3, y3 = self.slice3(x), self.slice3(y)\n",
        "        loss += F.l1_loss(x3, y3)\n",
        "\n",
        "        x4, y4 = self.slice4(x), self.slice4(y)\n",
        "        loss += F.l1_loss(x4, y4)\n",
        "\n",
        "        x5, y5 = self.slice5(x), self.slice5(y)\n",
        "        loss += F.l1_loss(x5, y5)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af8d487c",
      "metadata": {
        "id": "af8d487c"
      },
      "outputs": [],
      "source": [
        "class TotalLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.L1Loss()\n",
        "        self.vgg = VGGLoss()\n",
        "        self.ms_ssim = MS_SSIM(data_range=1.0, channel=1)\n",
        "\n",
        "    def forward(self, gen_ct, real_ct, D_real, D_fake, D):\n",
        "        # Reconstruction Losses\n",
        "        l1_loss = self.l1(gen_ct, real_ct)\n",
        "        ms_ssim_loss = 1 - self.ms_ssim(gen_ct, real_ct)\n",
        "        vgg_loss = self.vgg(gen_ct, real_ct)\n",
        "\n",
        "        # Adversarial Loss\n",
        "        adv_loss = -torch.mean(D_fake)\n",
        "\n",
        "        # Gradient Penalty\n",
        "        gp = self._gradient_penalty(D, real_ct, gen_ct.detach())\n",
        "\n",
        "        return 100*l1_loss + ms_ssim_loss + 0.1*vgg_loss + 10*(adv_loss + gp)\n",
        "\n",
        "    def _gradient_penalty(self, D, real, fake):\n",
        "        alpha = torch.rand(real.size(0), 1, 1, 1, device=real.device)\n",
        "        interpolates = (alpha * real + ((1 - alpha) * fake)).requires_grad_(True)\n",
        "        #d_interpolates = D(interpolates)\n",
        "        d_interpolates = D(interpolates).view(-1)\n",
        "\n",
        "        gradients = torch.autograd.grad(\n",
        "            outputs=d_interpolates,\n",
        "            inputs=interpolates,\n",
        "            grad_outputs=torch.ones_like(d_interpolates),\n",
        "            create_graph=True,\n",
        "            retain_graph=True\n",
        "        )[0]\n",
        "        return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "def psnr(output, target):\n",
        "    \"\"\"Compute PSNR between [-1,1] normalized tensors\"\"\"\n",
        "    output = (output + 1) / 2  # [-1,1] → [0,1]\n",
        "    target = (target + 1) / 2\n",
        "    mse = torch.mean((output - target) ** 2)\n",
        "    mse = torch.clamp(mse, min=1e-8)  # Avoid division by zero\n",
        "    return 20 * torch.log10(1.0 / torch.sqrt(mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6b9f01f",
      "metadata": {
        "id": "e6b9f01f"
      },
      "source": [
        "#### 6. Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "603a4214",
      "metadata": {
        "id": "603a4214"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    # 1. Get file splits from the processed PET directory (paired with CT files)\n",
        "    train_files, val_files, test_files = get_file_splits(processed_pet_dir, test_size=0.1, val_size=0.1)\n",
        "\n",
        "    # 2. Create Dataset objects for training, validation, and testing.\n",
        "    train_dataset = QinBreastDataset(train_files, processed_pet_dir, processed_ct_dir, transform=None)\n",
        "    val_dataset   = QinBreastDataset(val_files,   processed_pet_dir, processed_ct_dir, transform=None)\n",
        "    test_dataset  = QinBreastDataset(test_files,  processed_pet_dir, processed_ct_dir, transform=None)\n",
        "\n",
        "    # 3. Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=4, shuffle=False, num_workers=2)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 4. Define device and initialize your model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Instantiate the Generator model (this should include the ViT block as needed)\n",
        "    generator = Generator().to(device)\n",
        "\n",
        "    # Optionally, if you're using a discriminator and other components, instantiate them here.\n",
        "    # For now, we focus on training the Generator model for a simple demo.\n",
        "\n",
        "    # 5. Setup optimizer and loss function (e.g., L1 loss for reconstruction)\n",
        "    optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    num_epochs = 10  # Adjust number of training epochs\n",
        "\n",
        "    # 6. Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        generator.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for pet_img, ct_img in train_loader:\n",
        "            # Convert numpy arrays to torch tensors and add a channel dimension if needed.\n",
        "            # Assuming the npy files are in shape [H, W]. We convert them to [B, 1, H, W]\n",
        "            pet_img = torch.tensor(pet_img, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "            ct_img = torch.tensor(ct_img, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: The Generator takes PET image and outputs a predicted CT image.\n",
        "            pred_ct = generator(pet_img)\n",
        "\n",
        "            # Compute loss between the predicted CT and the real CT image.\n",
        "            loss = criterion(pred_ct, ct_img)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0083d84c",
      "metadata": {
        "id": "0083d84c"
      },
      "source": [
        "####  7. Execute Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c3682ac",
      "metadata": {
        "id": "1c3682ac"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
