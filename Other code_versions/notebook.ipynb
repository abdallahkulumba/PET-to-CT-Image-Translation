{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ae22b9",
   "metadata": {},
   "source": [
    "### Complete PET-to-CT Translation Pipeline\n",
    " **Architecture**: ResNet-34 Encoder + ViT Bottleneck + CNN Decoder  \n",
    " **Features**:\n",
    " - TCIA API download\n",
    " - NPY/PNG preprocessing (7GB storage)\n",
    " - Mixed precision training\n",
    " - Multi-scale SSIM loss\n",
    " - Model checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc975934",
   "metadata": {},
   "source": [
    "### 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49730fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydicom numpy pillow tqdm requests torch torchvision pytorch-msssim einops kaggle --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1834e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from pytorch_msssim import MS_SSIM\n",
    "from einops import rearrange\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358ae7c",
   "metadata": {},
   "source": [
    "#### 1. Download QIN-Breast from TCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f790ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_qin_breast(api_key, save_dir=\"/content/QIN-Breast_RAW\"):\n",
    "    \"\"\"Downloads DICOM files using TCIA API\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    #auth_url = f\"https://services.cancerimagingarchive.net/nbia-api/services/getToken?username={api_key}\"\n",
    "    #token = requests.get(auth_url).text.strip('\"')\n",
    "    #headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    \n",
    "    # Get list of series\n",
    "    series_url = \"https://services.cancerimagingarchive.net/nbia-api/services/getSeries?Collection=QIN-Breast\"\n",
    "    series_data = requests.get(series_url).json()\n",
    "    \n",
    "    # Download each DICOM\n",
    "    for series in tqdm(series_data, desc=\"Downloading\"):\n",
    "        series_uid = series[\"SeriesInstanceUID\"]\n",
    "        images_url = f\"https://services.cancerimagingarchive.net/nbia-api/services/getImage?SeriesInstanceUID={series_uid}\"\n",
    "        images = requests.get(images_url).json()\n",
    "        \n",
    "        for img in images:\n",
    "            img_url = f\"{images_url}&ImageInstanceUID={img['ImageInstanceUID']}\"\n",
    "            img_data = requests.get(img_url).content\n",
    "            os.makedirs(os.path.join(save_dir, series[\"PatientID\"]), exist_ok=True)\n",
    "            with open(os.path.join(save_dir, series[\"PatientID\"], f\"{img['ImageInstanceUID']}.dcm\"), \"wb\") as f:\n",
    "                f.write(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f0a97",
   "metadata": {},
   "source": [
    "#### 2. Preprocess to NPY/PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fa964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dicom_file(args):\n",
    "    \"\"\"Converts DICOM to normalized numpy array\"\"\"\n",
    "    dicom_path, output_dir = args\n",
    "    try:\n",
    "        dicom = pydicom.dcmread(dicom_path)\n",
    "        img = dicom.pixel_array.astype(np.float32)\n",
    "        \n",
    "        # Modality-specific normalization\n",
    "        if \"CT\" in dicom.Modality:\n",
    "            img = (img - img.min()) / (img.max() - img.min())  # [0,1]\n",
    "        elif \"PT\" in dicom.Modality:\n",
    "            img = (img + 1000) / 2000  # Approximate SUV scaling\n",
    "        \n",
    "        # Save as NPY\n",
    "        np.save(os.path.join(output_dir, f\"{dicom.Modality}_{dicom.PatientID}_{dicom.SOPInstanceUID}.npy\"), img)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dicom_path}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19328e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(raw_dir=\"/content/QIN-Breast_RAW\", \n",
    "                      processed_dir=\"/content/QIN-Breast_PROCESSED\"):\n",
    "    \"\"\"Parallel DICOM to NPY conversion\"\"\"\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    dicom_files = []\n",
    "    \n",
    "    for root, _, files in os.walk(raw_dir):\n",
    "        dicom_files.extend([os.path.join(root, f) for f in files if f.endswith(\".dcm\")])\n",
    "    \n",
    "    # Process in parallel\n",
    "    with Pool(4) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(process_dicom_file, [(f, processed_dir) for f in dicom_files]),\n",
    "            total=len(dicom_files),\n",
    "            desc=\"Preprocessing\"\n",
    "        ))\n",
    "    \n",
    "    print(f\"Successfully processed {sum(results)}/{len(dicom_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a sample\n",
    "files = os.listdir(processed_dir)\n",
    "if not files:\n",
    "    raise ValueError(\"No processed files found in directory\")\n",
    "npy_files = [f for f in os.listdir(processed_dir) if f.endswith(\".npy\")]\n",
    "if not npy_files:\n",
    "    raise ValueError(\"No .npy files found in processed directory\")\n",
    "       \n",
    "sample_npy = os.path.join(processed_dir, os.listdir(processed_dir)[0])\n",
    "sample = np.load(sample_npy)\n",
    "print(f\"Shape: {sample.shape}, Range: [{sample.min():.2f}, {sample.max():.2f}]\")\n",
    "plt.imshow(sample, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af104b",
   "metadata": {},
   "source": [
    "#### 3. Dataset splitting and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_splits(processed_dir, test_size=0.15, val_size=0.15):\n",
    "    \"\"\"Patient-wise splitting (prevents data leakage)\"\"\"\n",
    "    # Extract unique patient IDs from filenames (format: Modality_PatientID_UID.npy)\n",
    "    all_files = os.listdir(processed_dir)\n",
    "    pet_files = [f for f in all_files if f.startswith(\"PT_\")]\n",
    "    patient_ids = list(set([f.split('_')[1] for f in pet_files]))\n",
    "    \n",
    "    # Split: Train -> Val/Test\n",
    "    train_ids, test_ids = train_test_split(patient_ids, test_size=test_size, random_state=42)\n",
    "    train_ids, val_ids = train_test_split(train_ids, test_size=val_size/(1-test_size), random_state=42)\n",
    "    \n",
    "    return train_ids, val_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QinBreastDataset(Dataset):\n",
    "    def __init__(self, root_dir, patient_ids=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.pairs = []\n",
    "        \n",
    "        # Get PET files filtered by patient IDs\n",
    "        all_pet = [f for f in os.listdir(root_dir) if f.startswith(\"PT_\")]\n",
    "        if patient_ids:\n",
    "            all_pet = [f for f in all_pet if f.split('_')[1] in patient_ids]\n",
    "        \n",
    "        # Create verified pairs\n",
    "        for pet_file in all_pet:\n",
    "            ct_file = pet_file.replace(\"PT_\", \"CT_\")\n",
    "            if os.path.exists(os.path.join(root_dir, ct_file)):\n",
    "                self.pairs.append((pet_file, ct_file))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)  #returns no. of PET-CT pairs avialble\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pet_file, ct_file = self.pairs[idx]\n",
    "        #pet = np.load(os.path.join(self.root_dir, pet_file))\n",
    "        #ct = np.load(os.path.join(self.root_dir, ct_file))\n",
    "        try:\n",
    "            pet = np.load(os.path.join(self.root_dir, pet_file))\n",
    "            ct = np.load(os.path.join(self.root_dir, ct_file))\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading files: {pet_file}, {ct_file}. {e}\")\n",
    "        \n",
    "        if self.transform:\n",
    "            pet = self.transform(pet)\n",
    "            ct = self.transform(ct)\n",
    "            \n",
    "        return pet, ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc5e25",
   "metadata": {},
   "source": [
    "#### 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# ======================\n",
    "\n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, dim=512, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(dim, heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),  #GEL nad Dropout for better stability\n",
    "            nn.Linear(dim*4, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        mlp_out = self.mlp(x)\n",
    "        return self.norm2(x + mlp_out)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder (ResNet-34)\n",
    "        #resnet = models.resnet34(pretrained=True)\n",
    "        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            *list(resnet.children())[1:-2]  # Remove original fc layer\n",
    "        )\n",
    "        \n",
    "        # ViT Bottleneck\n",
    "        self.vit = nn.Sequential(\n",
    "            ViTBlock(dim=512),\n",
    "            #ViTBlock(dim=512),\n",
    "           # ViTBlock(dim=512)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        b, c, h, w = x.shape\n",
    "        x = rearrange(x, 'b c h w -> (h w) b c')\n",
    "        x = self.vit(x)\n",
    "        x = rearrange(x, '(h w) b c -> b c h w', h=h, w=w)\n",
    "        #return self.decoder(x)\n",
    "        return self.decoder(x.to(device))  # # Move output tensor back to GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd988e2",
   "metadata": {},
   "source": [
    "Multi-Scale Discriminator is designed to assess images at different resolutions, improving adversarial learning stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=1):\n",
    "        super().__init__()\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            self._make_discriminator(input_channels, 64),\n",
    "            self._make_discriminator(input_channels, 32),\n",
    "            self._make_discriminator(input_channels, 16)\n",
    "        ])\n",
    "        \n",
    "    def _make_discriminator(self, in_ch, base_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(in_ch, base_ch, 4, 2, 1)),  #Improves training stability by constraining weight norms.\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(base_ch, base_ch*2, 4, 2, 1)),\n",
    "            nn.InstanceNorm2d(base_ch*2),         #Helps normalize features, preventing vanishing or exploding gradients.\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(base_ch*2, 1, 4, 1, 1)),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        x = x.to(device)  # Ensure tensor is on GPU\n",
    "        for disc in self.discriminators:\n",
    "            outputs.append(disc(x))\n",
    "            #x = nn.functional.interpolate(x, scale_factor=0.5, mode='bilinear')\n",
    "            x = nn.functional.interpolate(x, scale_factor=0.5, mode='nearest')\n",
    "         #return torch.cat(outputs, dim=1)\n",
    "        return torch.cat(outputs, dim=1).to(device)  # Keep output on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c0f8c",
   "metadata": {},
   "source": [
    "####  5. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchvision.models import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8d487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.vgg = VGGLoss()\n",
    "        self.ms_ssim = MS_SSIM(data_range=1.0, channel=1)\n",
    "        \n",
    "    def forward(self, gen_ct, real_ct, D_real, D_fake, D):\n",
    "        # Reconstruction Losses\n",
    "        l1_loss = self.l1(gen_ct, real_ct)\n",
    "        ms_ssim_loss = 1 - self.ms_ssim(gen_ct, real_ct)\n",
    "        vgg_loss = self.vgg(gen_ct, real_ct)\n",
    "        \n",
    "        # Adversarial Loss\n",
    "        adv_loss = -torch.mean(D_fake)\n",
    "        \n",
    "        # Gradient Penalty\n",
    "        gp = self._gradient_penalty(D, real_ct, gen_ct.detach())\n",
    "        \n",
    "        return 100*l1_loss + ms_ssim_loss + 0.1*vgg_loss + 10*(adv_loss + gp)\n",
    "    \n",
    "    def _gradient_penalty(self, D, real, fake):\n",
    "        alpha = torch.rand(real.size(0), 1, 1, 1, device=real.device)\n",
    "        interpolates = (alpha * real + ((1 - alpha) * fake)).requires_grad_(True)\n",
    "        #d_interpolates = D(interpolates)\n",
    "        d_interpolates = D(interpolates).view(-1)\n",
    "\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones_like(d_interpolates),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "def psnr(output, target):\n",
    "    \"\"\"Compute PSNR between [-1,1] normalized tensors\"\"\"\n",
    "    output = (output + 1) / 2  # [-1,1] → [0,1]\n",
    "    target = (target + 1) / 2\n",
    "    mse = torch.mean((output - target) ** 2)\n",
    "    mse = torch.clamp(mse, min=1e-8)  # Avoid division by zero\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b9f01f",
   "metadata": {},
   "source": [
    "#### 6. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Initialize\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    G = Generator().to(device)\n",
    "    D = MultiScaleDiscriminator().to(device)\n",
    "    opt_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    opt_D = torch.optim.Adam(D.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "    criterion = TotalLoss()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Initialize SummaryWriter for TensorBoard logging\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    # Data\n",
    "    train_ids, val_ids, test_ids = get_patient_splits(\"/content/QIN-Breast_PROCESSED\")\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    eval_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    \n",
    "    train_dataset = QinBreastDataset(\"/content/QIN-Breast_PROCESSED\", train_ids, train_transform)\n",
    "    val_dataset = QinBreastDataset(\"/content/QIN-Breast_PROCESSED\", val_ids, eval_transform)\n",
    "    test_dataset = QinBreastDataset(\"/content/QIN-Breast_PROCESSED\", test_ids, eval_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0) # batch_size=16. num_workers=4\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    # Training\n",
    "    best_val_loss = float('inf')\n",
    "    global_step = 0  # Counter for logging iterations\n",
    "    for epoch in range(15):\n",
    "        # Training\n",
    "        G.train()\n",
    "        D.train()\n",
    "        train_losses = []\n",
    "        #for pet, ct in tqdm(train_loader, desc=f\"Train Epoch {epoch}\"):\n",
    "        for i, (pet, ct) in enumerate(train_loader):\n",
    "            if i >= 10:  # Limit to 10 batches for demonstration\n",
    "                break\n",
    "\n",
    "            pet, ct = pet.to(device), ct.to(device)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            opt_D.zero_grad()\n",
    "            with autocast():\n",
    "                fake_ct = G(pet)\n",
    "                D_real = D(ct)\n",
    "                D_fake = D(fake_ct.detach())\n",
    "                loss_D = criterion(fake_ct, ct, D_real, D_fake, D)\n",
    "            scaler.scale(loss_D).backward()\n",
    "            scaler.step(opt_D)\n",
    "            \n",
    "            # Train Generator\n",
    "            opt_G.zero_grad()\n",
    "            with autocast():\n",
    "                fake_ct = G(pet)\n",
    "                D_fake = D(fake_ct)\n",
    "                loss_G = criterion(fake_ct, ct, D_real, D_fake, D)\n",
    "            scaler.scale(loss_G).backward()\n",
    "            scaler.step(opt_G)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_losses.append(loss_G.item())\n",
    "             # Log the losses for this batch\n",
    "            writer.add_scalar(\"Train/Generator Loss\", loss_G.item(), global_step)\n",
    "            writer.add_scalar(\"Train/Discriminator Loss\", loss_D.item(), global_step)\n",
    "            global_step += 1  # Increment global step for TensorBoard logging\n",
    "        \n",
    "        # Validation\n",
    "        G.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            #for pet, ct in tqdm(val_loader, desc=\"Validating\"):\n",
    "            for i, (pet, ct) in enumerate(val_loader):\n",
    "                if i >= 10:  # Process only 2 batches from validation set\n",
    "                   break\n",
    "                pet, ct = pet.to(device), ct.to(device)\n",
    "                fake_ct = G(pet)\n",
    "                loss = criterion(fake_ct, ct, D(ct), D(fake_ct), D)\n",
    "                #change by copilot # Adjust TotalLoss to allow this, or use an alternative loss function\n",
    "                #loss = criterion(fake_ct, ct)  \n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        writer.add_scalar(\"Validation/Loss\", avg_val_loss, epoch)\n",
    "        print(f\"Epoch {epoch} | Train Loss: {np.mean(train_losses):.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Visualization & Evaluation Logging\n",
    "        # --------------------\n",
    "        # Log a few sample images from the validation set once every epoch\n",
    "        # (or every few epochs if desired)\n",
    "        sample_pet, sample_ct = next(iter(val_loader))\n",
    "        sample_pet = sample_pet.to(device)\n",
    "        with torch.no_grad():\n",
    "            sample_fake_ct = G(sample_pet)\n",
    "        # Convert images to [0,1] range for viewing if they were normalized to [-1,1]\n",
    "        sample_fake_ct_img = (sample_fake_ct + 1) / 2.0\n",
    "        sample_ct_img = (sample_ct + 1) / 2.0\n",
    "\n",
    "        # Log images to TensorBoard under \"Evaluation/Real_CT\" and \"Evaluation/Fake_CT\"\n",
    "        writer.add_images(\"Evaluation/Real_CT\", sample_ct_img, epoch)\n",
    "        writer.add_images(\"Evaluation/Fake_CT\", sample_fake_ct_img, epoch)\n",
    "\n",
    "        # You can also log evaluation metrics like PSNR or SSIM if computed per epoch\n",
    "        # For example:\n",
    "        # epoch_psnr = ...  # Compute PSNR value across the validation set\n",
    "        # writer.add_scalar(\"Evaluation/PSNR\", epoch_psnr, epoch)\n",
    "        \n",
    "        # Save best model\n",
    "        #if avg_val_loss < best_val_loss:\n",
    "           # best_val_loss = avg_val_loss\n",
    "           # torch.save(G.state_dict(), \"best_generator.pth\")\n",
    "          #  print(\"Saved new best model!\")\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"G_state_dict\": G.state_dict(),\n",
    "            \"D_state_dict\": D.state_dict(),\n",
    "            \"G_optimizer\": opt_G.state_dict(),\n",
    "            \"D_optimizer\": opt_D.state_dict()\n",
    "                }, \"best_model.pth\")\n",
    "             print(\"Saved new best model!\")\n",
    "\n",
    "    \n",
    "    # Final Test\n",
    "    G.load_state_dict(torch.load(\"best_generator.pth\"))\n",
    "    G.eval()\n",
    "    test_psnr = []\n",
    "    test_ssim = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #for pet, ct in tqdm(test_loader, desc=\"Testing\"):\n",
    "        for i, (pet, ct) in enumerate(test_loader):\n",
    "              if i >= 10:  # Process only 2 batches from test set\n",
    "                   break\n",
    "            pet, ct = pet.to(device), ct.to(device)\n",
    "            fake_ct = G(pet)\n",
    "            \n",
    "            # Convert to [0,1] for metrics\n",
    "            fake_ct = (fake_ct + 1) / 2\n",
    "            ct = (ct + 1) / 2\n",
    "            \n",
    "            test_psnr.append(psnr(fake_ct, ct).cpu().numpy())\n",
    "            test_ssim.append(ms_ssim(fake_ct, ct, data_range=1.0).cpu().numpy())\n",
    "    \n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"PSNR: {np.mean(test_psnr):.2f} ± {np.std(test_psnr):.2f} dB\")\n",
    "    print(f\"SSIM: {np.mean(test_ssim):.4f} ± {np.std(test_ssim):.4f}\")\n",
    "\n",
    "    # Make sure to close the writer after training:\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0083d84c",
   "metadata": {},
   "source": [
    "####  7. Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Step 1-2: Download and preprocess (one-time)\n",
    "    if not os.path.exists(\"/content/QIN-Breast_PROCESSED\"):\n",
    "        download_qin_breast(api_key=\"YOUR_TCIA_KEY\")\n",
    "        preprocess_dataset()\n",
    "    \n",
    "    # Step 3-6: Train\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
