{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "48ae22b9",
      "metadata": {
        "id": "48ae22b9"
      },
      "source": [
        "### Complete PET-to-CT Translation Pipeline\n",
        " **Architecture**: ResNet-34 Encoder + ViT Bottleneck + CNN Decoder  \n",
        " **Features**:\n",
        " - TCIA API download\n",
        " - NPY/PNG preprocessing (7GB storage)\n",
        " - Mixed precision training\n",
        " - Multi-scale SSIM loss\n",
        " - Model checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc975934",
      "metadata": {
        "id": "bc975934"
      },
      "source": [
        "### 0. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "49730fcb",
      "metadata": {
        "id": "49730fcb"
      },
      "outputs": [],
      "source": [
        "%pip install pydicom numpy pillow tqdm requests torch torchvision pytorch-msssim einops kaggle --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install optuna\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3JAnVHOkrg6",
        "outputId": "8787bc55-7d62-4457-9696-3204121970a5"
      },
      "id": "Q3JAnVHOkrg6",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a1834e3d",
      "metadata": {
        "id": "a1834e3d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool\n",
        "from pytorch_msssim import MS_SSIM\n",
        "from einops import rearrange\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import optuna\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0358ae7c",
      "metadata": {
        "id": "0358ae7c"
      },
      "source": [
        "#### 1. Download QIN-Breast from TCIA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Input directories for your original DICOM files\n",
        "pet_dir = '/content/drive/MyDrive/PIX/PET'\n",
        "ct_dir  = '/content/drive/MyDrive/PIX/CT'\n",
        "\n",
        "# Output directories for processed .npy files\n",
        "processed_pet_dir = '/content/drive/MyDrive/PIX/PET_P'\n",
        "processed_ct_dir  = '/content/drive/MyDrive/PIX/CT_P'\n"
      ],
      "metadata": {
        "id": "y5HeVze0IN64"
      },
      "id": "y5HeVze0IN64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7f790ac7",
      "metadata": {
        "id": "7f790ac7"
      },
      "outputs": [],
      "source": [
        "\"\"\"import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "def download_qin_breast(destination=\"/content/QIN-Breast_RAW\"):\n",
        "\n",
        "    #Downloads a sample of QIN Breast DCE-MRI dataset using the TCIA API and extracts it.\n",
        "    #Requires internet access.\n",
        "\n",
        "    os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "    # TCIA base API\n",
        "    BASE_URL = \"https://services.cancerimagingarchive.net/services/v4/TCIA/query\"\n",
        "    COLLECTION = \"QIN-Breast\"\n",
        "\n",
        "    # Step 1: List all studies in the collection\n",
        "    study_url = f\"{BASE_URL}/getSeries?Collection={COLLECTION}\"\n",
        "    response = requests.get(study_url)\n",
        "    if not response.ok:\n",
        "        raise Exception(\"Failed to fetch studies from TCIA\")\n",
        "\n",
        "    series_list = response.json()\n",
        "\n",
        "    # Step 2: Pick first N series (for demo purposes)\n",
        "    series_instances = [series[\"SeriesInstanceUID\"] for series in series_list[:20]]\n",
        "\n",
        "\n",
        "    for uid in series_instances:\n",
        "        print(f\"Downloading series: {uid}\")\n",
        "        download_url = f\"https://services.cancerimagingarchive.net/services/v4/TCIA/query/getImage?SeriesInstanceUID={uid}\"\n",
        "        out_path = os.path.join(destination, f\"{uid}.zip\")\n",
        "\n",
        "        # Download the ZIP\n",
        "        with requests.get(download_url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(out_path, 'wb') as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "        # Extract the ZIP\n",
        "        extract_dir = os.path.join(destination, uid)\n",
        "        os.makedirs(extract_dir, exist_ok=True)\n",
        "        with zipfile.ZipFile(out_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        os.remove(out_path)\n",
        "\n",
        "    print(\"Download complete and extracted to:\", destination)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "171f0a97",
      "metadata": {
        "id": "171f0a97"
      },
      "source": [
        "#### 2. Preprocess to NPY/PNG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pydicom\n",
        "import numpy as np\n",
        "\n",
        "def process_dicom_file(args):\n",
        "    \"\"\"\n",
        "    Convert a DICOM (.dcm) file to a NumPy (.npy) file.\n",
        "\n",
        "    Args:\n",
        "        args (tuple): Contains:\n",
        "                      - dicom_file (str): Full path to the DICOM file.\n",
        "                      - output_dir (str): Directory where the .npy file will be saved.\n",
        "    \"\"\"\n",
        "    dicom_file, output_dir = args\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Read the DICOM file\n",
        "        ds = pydicom.dcmread(dicom_file)\n",
        "        img_array = ds.pixel_array  # Extract pixel data\n",
        "\n",
        "        # Generate output filename (change .dcm to .npy)\n",
        "        base_name = os.path.basename(dicom_file)\n",
        "        output_filename = os.path.splitext(base_name)[0] + '.npy'\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        # Save the image array as a .npy file\n",
        "        np.save(output_path, img_array)\n",
        "        print(f\"Converted '{dicom_file}' to '{output_path}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {dicom_file}: {e}\")\n",
        "\n",
        "# Set your input directories (as provided) for PET and CT folders:\n",
        "pet_dir = '/content/drive/MyDrive/PIX/PET'\n",
        "ct_dir = '/content/drive/MyDrive/PIX/CT'\n",
        "\n",
        "# Set your desired output directories for the processed files\n",
        "output_pet_dir = '/content/drive/MyDrive/PIX/PET_P'\n",
        "output_ct_dir  = '/content/drive/MyDrive/PIX/CT_P'\n",
        "\n",
        "# List all DICOM files from each modality folder\n",
        "pet_files = sorted([os.path.join(pet_dir, f) for f in os.listdir(pet_dir) if f.endswith('.dcm')])\n",
        "ct_files  = sorted([os.path.join(ct_dir, f)  for f in os.listdir(ct_dir)  if f.endswith('.dcm')])\n",
        "\n",
        "# Process PET files\n",
        "print(\"Processing PET files...\")\n",
        "for dicom_file in pet_files:\n",
        "    process_dicom_file((dicom_file, output_pet_dir))\n",
        "\n",
        "# Process CT files\n",
        "print(\"Processing CT files...\")\n",
        "for dicom_file in ct_files:\n",
        "    process_dicom_file((dicom_file, output_ct_dir))\n"
      ],
      "metadata": {
        "id": "30vN-OOMO0mi"
      },
      "id": "30vN-OOMO0mi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "604fa964",
      "metadata": {
        "id": "604fa964"
      },
      "outputs": [],
      "source": [
        "\"\"\"\"def process_dicom_file(args):\n",
        "    Converts DICOM to normalized numpy array\n",
        "    dicom_path, output_dir = args\n",
        "    try:\n",
        "        dicom = pydicom.dcmread(dicom_path)\n",
        "        img = dicom.pixel_array.astype(np.float32)\n",
        "\n",
        "        # Modality-specific normalization\n",
        "        if \"CT\" in dicom.Modality:\n",
        "            img = (img - img.min()) / (img.max() - img.min())  # [0,1]\n",
        "        elif \"PT\" in dicom.Modality:\n",
        "            img = (img + 1000) / 2000  # Approximate SUV scaling\n",
        "\n",
        "        # Save as NPY\n",
        "        np.save(os.path.join(output_dir, f\"{dicom.Modality}_{dicom.PatientID}_{dicom.SOPInstanceUID}.npy\"), img)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {dicom_path}: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c19328e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "c19328e6",
        "outputId": "1b35f3c6-2b96-4888-9f51-e219a023ed3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' def preprocess_dataset(raw_dir=\"/content/QIN-Breast_RAW\", \\n                      processed_dir=\"/content/QIN-Breast_PROCESSED\"):\\n    #Parallel DICOM to NPY conversion\\n    os.makedirs(processed_dir, exist_ok=True)\\n    dicom_files = []\\n    \\n    for root, _, files in os.walk(raw_dir):\\n        dicom_files.extend([os.path.join(root, f) for f in files if f.endswith(\".dcm\")])\\n    \\n    # Process in parallel\\n    with Pool(4) as pool:\\n        results = list(tqdm(\\n            pool.imap(process_dicom_file, [(f, processed_dir) for f in dicom_files]),\\n            total=len(dicom_files),\\n            desc=\"Preprocessing\"\\n        ))\\n    \\n    print(f\"Successfully processed {sum(results)}/{len(dicom_files)} files\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\"\"\" def preprocess_dataset(raw_dir=\"/content/QIN-Breast_RAW\",\n",
        "                      processed_dir=\"/content/QIN-Breast_PROCESSED\"):\n",
        "    #Parallel DICOM to NPY conversion\n",
        "    os.makedirs(processed_dir, exist_ok=True)\n",
        "    dicom_files = []\n",
        "\n",
        "    for root, _, files in os.walk(raw_dir):\n",
        "        dicom_files.extend([os.path.join(root, f) for f in files if f.endswith(\".dcm\")])\n",
        "\n",
        "    # Process in parallel\n",
        "    with Pool(4) as pool:\n",
        "        results = list(tqdm(\n",
        "            pool.imap(process_dicom_file, [(f, processed_dir) for f in dicom_files]),\n",
        "            total=len(dicom_files),\n",
        "            desc=\"Preprocessing\"\n",
        "        ))\n",
        "\n",
        "    print(f\"Successfully processed {sum(results)}/{len(dicom_files)} files\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import shutil\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "def preprocess_dataset(raw_dir=\"/content/QIN-Breast_RAW\", processed_dir=\"/content/QIN-Breast_PROCESSED\"):\n",
        "    \"\"\"\n",
        "    Converts DICOM files into normalized NumPy arrays and saves them in processed_dir.\n",
        "    Filenames follow: PT_<PatientID>_<UID>.npy or CT_<PatientID>_<UID>.npy\n",
        "    \"\"\"\n",
        "    os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "    series_dirs = glob(os.path.join(raw_dir, \"*\"))\n",
        "    for series_path in tqdm(series_dirs, desc=\"Preprocessing series\"):\n",
        "        dicom_files = glob(os.path.join(series_path, \"*.dcm\"))\n",
        "        if len(dicom_files) == 0:\n",
        "            continue\n",
        "\n",
        "        # Try reading the first DICOM file\n",
        "        try:\n",
        "            sample = pydicom.dcmread(dicom_files[0], force=True)\n",
        "            modality = sample.Modality.upper()\n",
        "            patient_id = sample.PatientID\n",
        "            series_uid = sample.SeriesInstanceUID\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to read metadata from {series_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Only process PET or CT\n",
        "        if modality not in [\"PT\", \"CT\"]:\n",
        "            continue\n",
        "\n",
        "        # Sort slices by InstanceNumber or SliceLocation\n",
        "        slices = []\n",
        "        for f in dicom_files:\n",
        "            try:\n",
        "                dcm = pydicom.dcmread(f, force=True)\n",
        "                slices.append(dcm)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        slices = sorted(slices, key=lambda s: getattr(s, 'InstanceNumber', 0))\n",
        "\n",
        "        # Stack into volume\n",
        "        try:\n",
        "            volume = np.stack([s.pixel_array for s in slices]).astype(np.float32)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to stack slices for {series_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Normalize (min-max)\n",
        "        volume -= np.min(volume)\n",
        "        volume /= np.max(volume) + 1e-8  # Avoid divide-by-zero\n",
        "\n",
        "        # Save as .npy\n",
        "        save_path = os.path.join(processed_dir, f\"{modality}_{patient_id}_{series_uid}.npy\")\n",
        "        np.save(save_path, volume)\n",
        "\n",
        "    print(f\"All done! Processed files saved to {processed_dir}\")\n"
      ],
      "metadata": {
        "id": "dzyclWZiAEk3"
      },
      "id": "dzyclWZiAEk3",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "72af104b",
      "metadata": {
        "id": "72af104b"
      },
      "source": [
        "#### 3. Dataset splitting and Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3ebd6819",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "3ebd6819",
        "outputId": "4dcebd75-c314-4850-b80d-c2279c0f8964"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' def get_patient_splits(processed_dir, test_size=0.15, val_size=0.15):\\n    #Patient-wise splitting (prevents data leakage)\\n    # Extract unique patient IDs from filenames (format: Modality_PatientID_UID.npy)\\n    all_files = os.listdir(processed_dir)\\n    pet_files = [f for f in all_files if f.startswith(\"PT_\")]\\n    patient_ids = list(set([f.split(\\'_\\')[1] for f in pet_files]))\\n    \\n    # Split: Train -> Val/Test\\n    train_ids, test_ids = train_test_split(patient_ids, test_size=test_size, random_state=42)\\n    train_ids, val_ids = train_test_split(train_ids, test_size=val_size/(1-test_size), random_state=42)\\n    \\n    return train_ids, val_ids, test_ids'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\"\"\" def get_patient_splits(processed_dir, test_size=0.15, val_size=0.15):\n",
        "    #Patient-wise splitting (prevents data leakage)\n",
        "    # Extract unique patient IDs from filenames (format: Modality_PatientID_UID.npy)\n",
        "    all_files = os.listdir(processed_dir)\n",
        "    pet_files = [f for f in all_files if f.startswith(\"PT_\")]\n",
        "    patient_ids = list(set([f.split('_')[1] for f in pet_files]))\n",
        "\n",
        "    # Split: Train -> Val/Test\n",
        "    train_ids, test_ids = train_test_split(patient_ids, test_size=test_size, random_state=42)\n",
        "    train_ids, val_ids = train_test_split(train_ids, test_size=val_size/(1-test_size), random_state=42)\n",
        "\n",
        "    return train_ids, val_ids, test_ids\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_patient_splits(processed_dir, test_size=0.15, val_size=0.15):\n",
        "    \"\"\"Patient-wise splitting (prevents data leakage)\"\"\"\n",
        "    all_files = os.listdir(processed_dir)\n",
        "    pet_files = [f for f in all_files if f.startswith(\"PT_\")]\n",
        "\n",
        "    if len(pet_files) == 0:\n",
        "        raise ValueError(\"No PET files found. Check if preprocessing ran and file naming is correct.\")\n",
        "\n",
        "    patient_ids = list(set([f.split('_')[1] for f in pet_files]))\n",
        "\n",
        "    if len(patient_ids) < 3:\n",
        "        raise ValueError(f\"Too few patients ({len(patient_ids)}). Need at least 3 to split into train/val/test.\")\n",
        "\n",
        "    # Split: Train -> Val/Test\n",
        "    train_ids, test_ids = train_test_split(patient_ids, test_size=test_size, random_state=42)\n",
        "    train_ids, val_ids = train_test_split(train_ids, test_size=val_size/(1-test_size), random_state=42)\n",
        "\n",
        "    return train_ids, val_ids, test_ids\n"
      ],
      "metadata": {
        "id": "tcehiE5KAQ0M"
      },
      "id": "tcehiE5KAQ0M",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a007b5b5",
      "metadata": {
        "id": "a007b5b5"
      },
      "outputs": [],
      "source": [
        "class QinBreastDataset(Dataset):\n",
        "    def __init__(self, root_dir, patient_ids=None, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.pairs = []\n",
        "\n",
        "        # Get PET files filtered by patient IDs\n",
        "        all_pet = [f for f in os.listdir(root_dir) if f.startswith(\"PT_\")]\n",
        "        if patient_ids:\n",
        "            all_pet = [f for f in all_pet if f.split('_')[1] in patient_ids]\n",
        "\n",
        "        # Create verified pairs\n",
        "        for pet_file in all_pet:\n",
        "            ct_file = pet_file.replace(\"PT_\", \"CT_\")\n",
        "            if os.path.exists(os.path.join(root_dir, ct_file)):\n",
        "                self.pairs.append((pet_file, ct_file))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)  #returns no. of PET-CT pairs avialble\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pet_file, ct_file = self.pairs[idx]\n",
        "        #pet = np.load(os.path.join(self.root_dir, pet_file))\n",
        "        #ct = np.load(os.path.join(self.root_dir, ct_file))\n",
        "        try:\n",
        "            pet = np.load(os.path.join(self.root_dir, pet_file))\n",
        "            ct = np.load(os.path.join(self.root_dir, ct_file))\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error loading files: {pet_file}, {ct_file}. {e}\")\n",
        "\n",
        "        if self.transform:\n",
        "            pet = self.transform(pet)\n",
        "            ct = self.transform(ct)\n",
        "\n",
        "        return pet, ct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdc5e25",
      "metadata": {
        "id": "3fdc5e25"
      },
      "source": [
        "#### 4. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0cf3ffd3",
      "metadata": {
        "id": "0cf3ffd3"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# ======================\n",
        "\n",
        "class ViTBlock(nn.Module):\n",
        "    def __init__(self, dim=512, heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(dim, heads, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, dim*4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),  #GEL nad Dropout for better stability\n",
        "            nn.Linear(dim*4, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attention(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        mlp_out = self.mlp(x)\n",
        "        return self.norm2(x + mlp_out)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder (ResNet-34)\n",
        "        #resnet = models.resnet34(pretrained=True)\n",
        "        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            *list(resnet.children())[1:-2]  # Remove original fc layer\n",
        "        )\n",
        "\n",
        "        # ViT Bottleneck\n",
        "        self.vit = nn.Sequential(\n",
        "            ViTBlock(dim=512),\n",
        "            #ViTBlock(dim=512),\n",
        "           # ViTBlock(dim=512)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        b, c, h, w = x.shape\n",
        "        x = rearrange(x, 'b c h w -> (h w) b c')\n",
        "        x = self.vit(x)\n",
        "        x = rearrange(x, '(h w) b c -> b c h w', h=h, w=w)\n",
        "        #return self.decoder(x)\n",
        "        return self.decoder(x.to(device))  # # Move output tensor back to GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfd988e2",
      "metadata": {
        "id": "cfd988e2"
      },
      "source": [
        "Multi-Scale Discriminator is designed to assess images at different resolutions, improving adversarial learning stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8311261f",
      "metadata": {
        "id": "8311261f"
      },
      "outputs": [],
      "source": [
        "class MultiScaleDiscriminator(nn.Module):\n",
        "    def __init__(self, input_channels=1):\n",
        "        super().__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            self._make_discriminator(input_channels, 64),\n",
        "            self._make_discriminator(input_channels, 32),\n",
        "            self._make_discriminator(input_channels, 16)\n",
        "        ])\n",
        "\n",
        "    def _make_discriminator(self, in_ch, base_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(in_ch, base_ch, 4, 2, 1)),  #Improves training stability by constraining weight norms.\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(base_ch, base_ch*2, 4, 2, 1)),\n",
        "            nn.InstanceNorm2d(base_ch*2),         #Helps normalize features, preventing vanishing or exploding gradients.\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(base_ch*2, 1, 4, 1, 1)),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        x = x.to(device)  # Ensure tensor is on GPU\n",
        "        for disc in self.discriminators:\n",
        "            outputs.append(disc(x))\n",
        "            #x = nn.functional.interpolate(x, scale_factor=0.5, mode='bilinear')\n",
        "            x = nn.functional.interpolate(x, scale_factor=0.5, mode='nearest')\n",
        "         #return torch.cat(outputs, dim=1)\n",
        "        return torch.cat(outputs, dim=1).to(device)  # Keep output on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6c0f8c",
      "metadata": {
        "id": "fb6c0f8c"
      },
      "source": [
        "####  5. Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5845ea47",
      "metadata": {
        "id": "5845ea47"
      },
      "outputs": [],
      "source": [
        "#from torchvision.models import vgg19\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg19, VGG19_Weights\n",
        "\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        # Load a pre-trained VGG19 model\n",
        "        vgg = vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        # Break the network into slices corresponding to different layers' outputs.\n",
        "        self.slice1 = nn.Sequential(*[vgg[x] for x in range(2)])\n",
        "        self.slice2 = nn.Sequential(*[vgg[x] for x in range(2, 7)])\n",
        "        self.slice3 = nn.Sequential(*[vgg[x] for x in range(7, 12)])\n",
        "        self.slice4 = nn.Sequential(*[vgg[x] for x in range(12, 21)])\n",
        "        self.slice5 = nn.Sequential(*[vgg[x] for x in range(21, 30)])\n",
        "\n",
        "        # Freeze the VGG parameters if not training them.\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Compute feature maps at various depths\n",
        "        loss = 0\n",
        "        x1, y1 = self.slice1(x), self.slice1(y)\n",
        "        loss += F.l1_loss(x1, y1)\n",
        "\n",
        "        x2, y2 = self.slice2(x), self.slice2(y)\n",
        "        loss += F.l1_loss(x2, y2)\n",
        "\n",
        "        x3, y3 = self.slice3(x), self.slice3(y)\n",
        "        loss += F.l1_loss(x3, y3)\n",
        "\n",
        "        x4, y4 = self.slice4(x), self.slice4(y)\n",
        "        loss += F.l1_loss(x4, y4)\n",
        "\n",
        "        x5, y5 = self.slice5(x), self.slice5(y)\n",
        "        loss += F.l1_loss(x5, y5)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "af8d487c",
      "metadata": {
        "id": "af8d487c"
      },
      "outputs": [],
      "source": [
        "class TotalLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.L1Loss()\n",
        "        self.vgg = VGGLoss()\n",
        "        self.ms_ssim = MS_SSIM(data_range=1.0, channel=1)\n",
        "\n",
        "    def forward(self, gen_ct, real_ct, D_real, D_fake, D):\n",
        "        # Reconstruction Losses\n",
        "        l1_loss = self.l1(gen_ct, real_ct)\n",
        "        ms_ssim_loss = 1 - self.ms_ssim(gen_ct, real_ct)\n",
        "        vgg_loss = self.vgg(gen_ct, real_ct)\n",
        "\n",
        "        # Adversarial Loss\n",
        "        adv_loss = -torch.mean(D_fake)\n",
        "\n",
        "        # Gradient Penalty\n",
        "        gp = self._gradient_penalty(D, real_ct, gen_ct.detach())\n",
        "\n",
        "        return 100*l1_loss + ms_ssim_loss + 0.1*vgg_loss + 10*(adv_loss + gp)\n",
        "\n",
        "    def _gradient_penalty(self, D, real, fake):\n",
        "        alpha = torch.rand(real.size(0), 1, 1, 1, device=real.device)\n",
        "        interpolates = (alpha * real + ((1 - alpha) * fake)).requires_grad_(True)\n",
        "        #d_interpolates = D(interpolates)\n",
        "        d_interpolates = D(interpolates).view(-1)\n",
        "\n",
        "        gradients = torch.autograd.grad(\n",
        "            outputs=d_interpolates,\n",
        "            inputs=interpolates,\n",
        "            grad_outputs=torch.ones_like(d_interpolates),\n",
        "            create_graph=True,\n",
        "            retain_graph=True\n",
        "        )[0]\n",
        "        return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "def psnr(output, target):\n",
        "    \"\"\"Compute PSNR between [-1,1] normalized tensors\"\"\"\n",
        "    output = (output + 1) / 2  # [-1,1] → [0,1]\n",
        "    target = (target + 1) / 2\n",
        "    mse = torch.mean((output - target) ** 2)\n",
        "    mse = torch.clamp(mse, min=1e-8)  # Avoid division by zero\n",
        "    return 20 * torch.log10(1.0 / torch.sqrt(mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6b9f01f",
      "metadata": {
        "id": "e6b9f01f"
      },
      "source": [
        "#### 6. Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "603a4214",
      "metadata": {
        "id": "603a4214"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    # Initialize\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    G = Generator().to(device)\n",
        "    D = MultiScaleDiscriminator().to(device)\n",
        "    opt_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "    opt_D = torch.optim.Adam(D.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "    criterion = TotalLoss()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Initialize SummaryWriter for TensorBoard logging\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    # Data\n",
        "    train_ids, val_ids, test_ids = get_patient_splits(\"/content/QIN-Breast_PROCESSED\")\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "    eval_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "\n",
        "    train_dataset = QinBreastDataset(\"/content/QIN-Breast_PROCESSED\", train_ids, train_transform)\n",
        "    val_dataset = QinBreastDataset(\"/content/QIN-Breast_PROCESSED\", val_ids, eval_transform)\n",
        "    test_dataset = QinBreastDataset(\"/content/QIN-Breast_PROCESSED\", test_ids, eval_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0) # batch_size=16. num_workers=4\n",
        "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "    # Training\n",
        "    best_val_loss = float('inf')\n",
        "    global_step = 0  # Counter for logging iterations\n",
        "    for epoch in range(15):\n",
        "        # Training\n",
        "        G.train()\n",
        "        D.train()\n",
        "        train_losses = []\n",
        "        #for pet, ct in tqdm(train_loader, desc=f\"Train Epoch {epoch}\"):\n",
        "        for i, (pet, ct) in enumerate(train_loader):\n",
        "            if i >= 10:  # Limit to 10 batches for demonstration\n",
        "                break\n",
        "\n",
        "            pet, ct = pet.to(device), ct.to(device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            opt_D.zero_grad()\n",
        "            with autocast():\n",
        "                fake_ct = G(pet)\n",
        "                D_real = D(ct)\n",
        "                D_fake = D(fake_ct.detach())\n",
        "                loss_D = criterion(fake_ct, ct, D_real, D_fake, D)\n",
        "            scaler.scale(loss_D).backward()\n",
        "            scaler.step(opt_D)\n",
        "\n",
        "            # Train Generator\n",
        "            opt_G.zero_grad()\n",
        "            with autocast():\n",
        "                fake_ct = G(pet)\n",
        "                D_fake = D(fake_ct)\n",
        "                loss_G = criterion(fake_ct, ct, D_real, D_fake, D)\n",
        "            scaler.scale(loss_G).backward()\n",
        "            scaler.step(opt_G)\n",
        "            scaler.update()\n",
        "\n",
        "            train_losses.append(loss_G.item())\n",
        "             # Log the losses for this batch\n",
        "            writer.add_scalar(\"Train/Generator Loss\", loss_G.item(), global_step)\n",
        "            writer.add_scalar(\"Train/Discriminator Loss\", loss_D.item(), global_step)\n",
        "            global_step += 1  # Increment global step for TensorBoard logging\n",
        "\n",
        "        # Validation\n",
        "        G.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            #for pet, ct in tqdm(val_loader, desc=\"Validating\"):\n",
        "            for i, (pet, ct) in enumerate(val_loader):\n",
        "                if i >= 10:  # Process only 2 batches from validation set\n",
        "                   break\n",
        "                pet, ct = pet.to(device), ct.to(device)\n",
        "                fake_ct = G(pet)\n",
        "                loss = criterion(fake_ct, ct, D(ct), D(fake_ct), D)\n",
        "                #change by copilot # Adjust TotalLoss to allow this, or use an alternative loss function\n",
        "                #loss = criterion(fake_ct, ct)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        writer.add_scalar(\"Validation/Loss\", avg_val_loss, epoch)\n",
        "        print(f\"Epoch {epoch} | Train Loss: {np.mean(train_losses):.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Visualization & Evaluation Logging\n",
        "        # --------------------\n",
        "        # Log a few sample images from the validation set once every epoch\n",
        "        # (or every few epochs if desired)\n",
        "        sample_pet, sample_ct = next(iter(val_loader))\n",
        "        sample_pet = sample_pet.to(device)\n",
        "        with torch.no_grad():\n",
        "            sample_fake_ct = G(sample_pet)\n",
        "        # Convert images to [0,1] range for viewing if they were normalized to [-1,1]\n",
        "        sample_fake_ct_img = (sample_fake_ct + 1) / 2.0\n",
        "        sample_ct_img = (sample_ct + 1) / 2.0\n",
        "\n",
        "        # Log images to TensorBoard under \"Evaluation/Real_CT\" and \"Evaluation/Fake_CT\"\n",
        "        writer.add_images(\"Evaluation/Real_CT\", sample_ct_img, epoch)\n",
        "        writer.add_images(\"Evaluation/Fake_CT\", sample_fake_ct_img, epoch)\n",
        "\n",
        "        # You can also log evaluation metrics like PSNR or SSIM if computed per epoch\n",
        "        # For example:\n",
        "        # epoch_psnr = ...  # Compute PSNR value across the validation set\n",
        "        # writer.add_scalar(\"Evaluation/PSNR\", epoch_psnr, epoch)\n",
        "\n",
        "        # Save best model\n",
        "        #if avg_val_loss < best_val_loss:\n",
        "           # best_val_loss = avg_val_loss\n",
        "           # torch.save(G.state_dict(), \"best_generator.pth\")\n",
        "          #  print(\"Saved new best model!\")\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"G_optimizer\": opt_G.state_dict(),\n",
        "            \"D_optimizer\": opt_D.state_dict()\n",
        "                }, \"best_model.pth\")\n",
        "            print(\"Saved new best model!\")\n",
        "\n",
        "\n",
        "    # Final Test\n",
        "    G.load_state_dict(torch.load(\"best_generator.pth\"))\n",
        "    G.eval()\n",
        "    test_psnr = []\n",
        "    test_ssim = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #for pet, ct in tqdm(test_loader, desc=\"Testing\"):\n",
        "        for i, (pet, ct) in enumerate(test_loader):\n",
        "              if i >= 10:  # Process only 2 batches from test set\n",
        "                   break\n",
        "              pet, ct = pet.to(device), ct.to(device)\n",
        "              fake_ct = G(pet)\n",
        "\n",
        "              # Convert to [0,1] for metrics\n",
        "              fake_ct = (fake_ct + 1) / 2\n",
        "              ct = (ct + 1) / 2\n",
        "\n",
        "              test_psnr.append(psnr(fake_ct, ct).cpu().numpy())\n",
        "              test_ssim.append(ms_ssim(fake_ct, ct, data_range=1.0).cpu().numpy())\n",
        "\n",
        "    print(f\"\\nFinal Test Results:\")\n",
        "    print(f\"PSNR: {np.mean(test_psnr):.2f} ± {np.std(test_psnr):.2f} dB\")\n",
        "    print(f\"SSIM: {np.mean(test_ssim):.4f} ± {np.std(test_ssim):.4f}\")\n",
        "\n",
        "    # Make sure to close the writer after training:\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH6dTHA-rPOx",
        "outputId": "26088661-cffe-45f6-8be3-720998649944"
      },
      "id": "UH6dTHA-rPOx",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0083d84c",
      "metadata": {
        "id": "0083d84c"
      },
      "source": [
        "####  7. Execute Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "196a6319",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "196a6319",
        "outputId": "1867073e-ee82-4bb0-fe00-871ddb32f9c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-fa80d62e99ce>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No PET files found. Check if preprocessing ran and file naming is correct.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ebf8530ee4db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Step 3-6: Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-fa80d62e99ce>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_patient_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/QIN-Breast_PROCESSED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     train_transform = transforms.Compose([\n",
            "\u001b[0;32m<ipython-input-12-a7f067bc5cc4>\u001b[0m in \u001b[0;36mget_patient_splits\u001b[0;34m(processed_dir, test_size, val_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpet_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No PET files found. Check if preprocessing ran and file naming is correct.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpatient_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpet_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No PET files found. Check if preprocessing ran and file naming is correct."
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Step 1-2: Download and preprocess (one-time)\n",
        "    if not os.path.exists(\"/content/QIN-Breast_PROCESSED\"):\n",
        "        download_qin_breast()\n",
        "        preprocess_dataset()\n",
        "\n",
        "    # Step 3-6: Train\n",
        "    train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "processed_dir = \"/content/QIN-Breast_PROCESSED\"\n",
        "files = os.listdir(processed_dir)\n",
        "\n",
        "print(\"Sample files:\", files[:10])\n",
        "print(\"Total files:\", len(files))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNDz9SgzBQZ0",
        "outputId": "b09f7052-0ec2-4814-d247-3d24c342939d"
      },
      "id": "uNDz9SgzBQZ0",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample files: ['CT_QIN-BREAST-01-0005_1.3.6.1.4.1.14519.5.2.1.8162.7003.184534544545899155746390970758.npy', 'CT_QIN-BREAST-01-0005_1.3.6.1.4.1.14519.5.2.1.8162.7003.194451068311289985577103378471.npy', 'CT_QIN-BREAST-01-0007_1.3.6.1.4.1.14519.5.2.1.8162.7003.843576143914773065152505268737.npy', 'CT_QIN-BREAST-01-0003_1.3.6.1.4.1.14519.5.2.1.8162.7003.154745635425029481935829340289.npy', 'CT_QIN-BREAST-01-0003_1.3.6.1.4.1.14519.5.2.1.8162.7003.304251602254400173448218579444.npy', 'CT_QIN-BREAST-01-0002_1.3.6.1.4.1.14519.5.2.1.8162.7003.135793651241397654453131409563.npy', 'CT_QIN-BREAST-01-0003_1.3.6.1.4.1.14519.5.2.1.8162.7003.187597513862753106512454405500.npy', 'CT_QIN-BREAST-01-0002_1.3.6.1.4.1.14519.5.2.1.8162.7003.129443967611089824437584755973.npy', 'CT_QIN-BREAST-01-0007_1.3.6.1.4.1.14519.5.2.1.8162.7003.838123888394889102410256573858.npy', 'CT_QIN-BREAST-01-0005_1.3.6.1.4.1.14519.5.2.1.8162.7003.287873048257814352778755034563.npy']\n",
            "Total files: 830\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}