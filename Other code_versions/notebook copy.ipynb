{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ae22b9",
   "metadata": {},
   "source": [
    "### Complete PET-to-CT Translation Pipeline\n",
    " **Architecture**: ResNet-34 Encoder + ViT Bottleneck + CNN Decoder  \n",
    " **Features**:\n",
    " - TCIA API download\n",
    " - NPY/PNG preprocessing (7GB storage)\n",
    " - Mixed precision training\n",
    " - Multi-scale SSIM loss\n",
    " - Model checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc975934",
   "metadata": {},
   "source": [
    "### 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49730fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydicom numpy pillow tqdm requests torch torchvision pytorch-msssim einops kaggle scikit-learn tensorboard pyyaml --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1834e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from pytorch_msssim import ms_ssim, SSIM\n",
    "from einops import rearrange\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7d1a3",
   "metadata": {},
   "source": [
    "#### 1. Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e292733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Hyperparameters\n",
    "        self.batch_size = 16\n",
    "        self.lr_g = 2e-4\n",
    "        self.lr_d = 1e-4\n",
    "        self.beta1 = 0.5\n",
    "        self.beta2 = 0.999\n",
    "        self.lambda_l1 = 100\n",
    "        self.lambda_ms_ssim = 1\n",
    "        self.lambda_vgg = 0.1\n",
    "        self.lambda_gp = 10\n",
    "        \n",
    "        # Training\n",
    "        self.epochs = 100\n",
    "        self.patience = 10\n",
    "        \n",
    "        # Paths\n",
    "        self.tb_log_dir = \"logs/exp1\"\n",
    "        self.model_dir = \"saved_models\"\n",
    "        self.processed_dir = \"/content/QIN-Breast_PROCESSED\"\n",
    "        \n",
    "    def save(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            yaml.dump(self.__dict__, f)\n",
    "            \n",
    "    def load(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            self.__dict__.update(yaml.safe_load(f))\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358ae7c",
   "metadata": {},
   "source": [
    "#### 1. Download QIN-Breast from TCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f790ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_qin_breast(api_key, save_dir=\"/content/QIN-Breast_RAW\"):\n",
    "    \"\"\"Downloads DICOM files using TCIA API\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    auth_url = f\"https://services.cancerimagingarchive.net/nbia-api/services/getToken?username={api_key}\"\n",
    "    token = requests.get(auth_url).text.strip('\"')\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    \n",
    "    # Get list of series\n",
    "    series_url = \"https://services.cancerimagingarchive.net/nbia-api/services/getSeries?Collection=QIN-Breast\"\n",
    "    series_data = requests.get(series_url, headers=headers).json()\n",
    "    \n",
    "    # Download each DICOM\n",
    "    for series in tqdm(series_data, desc=\"Downloading\"):\n",
    "        series_uid = series[\"SeriesInstanceUID\"]\n",
    "        images_url = f\"https://services.cancerimagingarchive.net/nbia-api/services/getImage?SeriesInstanceUID={series_uid}\"\n",
    "        images = requests.get(images_url, headers=headers).json()\n",
    "        \n",
    "        for img in images:\n",
    "            img_url = f\"{images_url}&ImageInstanceUID={img['ImageInstanceUID']}\"\n",
    "            img_data = requests.get(img_url, headers=headers).content\n",
    "            os.makedirs(os.path.join(save_dir, series[\"PatientID\"]), exist_ok=True)\n",
    "            with open(os.path.join(save_dir, series[\"PatientID\"], f\"{img['ImageInstanceUID']}.dcm\"), \"wb\") as f:\n",
    "                f.write(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f0a97",
   "metadata": {},
   "source": [
    "#### 2. Preprocess to NPY/PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fa964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dicom_file(args):\n",
    "    \"\"\"Converts DICOM to normalized numpy array\"\"\"\n",
    "    dicom_path, output_dir = args\n",
    "    try:\n",
    "        dicom = pydicom.dcmread(dicom_path)\n",
    "        img = dicom.pixel_array.astype(np.float32)\n",
    "        \n",
    "        # Modality-specific normalization\n",
    "        if \"CT\" in dicom.Modality:\n",
    "            img = (img - img.min()) / (img.max() - img.min())  # [0,1]\n",
    "        elif \"PT\" in dicom.Modality:\n",
    "            img = (img + 1000) / 2000  # Approximate SUV scaling\n",
    "        \n",
    "        # Save as NPY\n",
    "        np.save(os.path.join(output_dir, f\"{dicom.Modality}_{dicom.PatientID}_{dicom.SOPInstanceUID}.npy\"), img)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dicom_path}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19328e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(raw_dir=\"/content/QIN-Breast_RAW\", \n",
    "                      processed_dir=\"/content/QIN-Breast_PROCESSED\"):\n",
    "    \"\"\"Parallel DICOM to NPY conversion\"\"\"\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    dicom_files = []\n",
    "    \n",
    "    for root, _, files in os.walk(raw_dir):\n",
    "        dicom_files.extend([os.path.join(root, f) for f in files if f.endswith(\".dcm\")])\n",
    "    \n",
    "    # Process in parallel\n",
    "    with Pool(4) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(process_dicom_file, [(f, processed_dir) for f in dicom_files]),\n",
    "            total=len(dicom_files),\n",
    "            desc=\"Preprocessing\"\n",
    "        ))\n",
    "    \n",
    "    print(f\"Successfully processed {sum(results)}/{len(dicom_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a sample\n",
    "sample_npy = os.path.join(processed_dir, os.listdir(processed_dir)[0])\n",
    "sample = np.load(sample_npy)\n",
    "print(f\"Shape: {sample.shape}, Range: [{sample.min():.2f}, {sample.max():.2f}]\")\n",
    "plt.imshow(sample, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af104b",
   "metadata": {},
   "source": [
    "#### 3. Dataset splitting and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_splits(processed_dir, test_size=0.15, val_size=0.15):\n",
    "    \"\"\"Patient-wise splitting (prevents data leakage)\"\"\"\n",
    "    # Extract unique patient IDs from filenames (format: Modality_PatientID_UID.npy)\n",
    "    all_files = os.listdir(processed_dir)\n",
    "    pet_files = [f for f in all_files if f.startswith(\"PT_\")]\n",
    "    patient_ids = list(set([f.split('_')[1] for f in pet_files))\n",
    "    \n",
    "    # Split: Train -> Val/Test\n",
    "    train_ids, test_ids = train_test_split(patient_ids, test_size=test_size, random_state=42)\n",
    "    train_ids, val_ids = train_test_split(train_ids, test_size=val_size/(1-test_size), random_state=42)\n",
    "    \n",
    "    return train_ids, val_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QinBreastDataset(Dataset):\n",
    "    def __init__(self, root_dir, patient_ids=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.pairs = []\n",
    "        \n",
    "        # Get PET files filtered by patient IDs\n",
    "        all_pet = [f for f in os.listdir(root_dir) if f.startswith(\"PT_\")]\n",
    "        if patient_ids:\n",
    "            all_pet = [f for f in all_pet if f.split('_')[1] in patient_ids]\n",
    "        \n",
    "        # Create verified pairs\n",
    "        for pet_file in all_pet:\n",
    "            ct_file = pet_file.replace(\"PT_\", \"CT_\")\n",
    "            if os.path.exists(os.path.join(root_dir, ct_file)):\n",
    "                self.pairs.append((pet_file, ct_file))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pet_file, ct_file = self.pairs[idx]\n",
    "        pet = np.load(os.path.join(self.root_dir, pet_file))\n",
    "        ct = np.load(os.path.join(self.root_dir, ct_file))\n",
    "        \n",
    "        if self.transform:\n",
    "            pet = self.transform(pet)\n",
    "            ct = self.transform(ct)\n",
    "            \n",
    "        return pet, ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc5e25",
   "metadata": {},
   "source": [
    "#### 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# ======================\n",
    "\n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, dim=512, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(dim, heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim*4, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        mlp_out = self.mlp(x)\n",
    "        return self.norm2(x + mlp_out)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder (ResNet-34)\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            *list(resnet.children())[1:-2]  # Remove original fc layer\n",
    "        )\n",
    "        \n",
    "        # ViT Bottleneck\n",
    "        self.vit = nn.Sequential(\n",
    "            ViTBlock(dim=512),\n",
    "            ViTBlock(dim=512),\n",
    "            ViTBlock(dim=512)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        b, c, h, w = x.shape\n",
    "        x = rearrange(x, 'b c h w -> (h w) b c')\n",
    "        x = self.vit(x)\n",
    "        x = rearrange(x, '(h w) b c -> b c h w', h=h, w=w)\n",
    "        return self.decoder(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=1):\n",
    "        super().__init__()\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            self._make_discriminator(input_channels, 64),\n",
    "            self._make_discriminator(input_channels, 32),\n",
    "            self._make_discriminator(input_channels, 16)\n",
    "        ])\n",
    "        \n",
    "    def _make_discriminator(self, in_ch, base_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(in_ch, base_ch, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(base_ch, base_ch*2, 4, 2, 1)),\n",
    "            nn.InstanceNorm2d(base_ch*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(base_ch*2, 1, 4, 1, 1)),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for disc in self.discriminators:\n",
    "            outputs.append(disc(x))\n",
    "            x = nn.functional.interpolate(x, scale_factor=0.5, mode='bilinear')\n",
    "        return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c0f8c",
   "metadata": {},
   "source": [
    "####  5. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8d487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.vgg = VGGLoss()\n",
    "        self.ms_ssim = MS_SSIM(data_range=1.0, channel=1)\n",
    "        \n",
    "    def forward(self, gen_ct, real_ct, D_real, D_fake, D):\n",
    "        # Reconstruction Losses\n",
    "        l1_loss = self.l1(gen_ct, real_ct)\n",
    "        ms_ssim_loss = 1 - self.ms_ssim(gen_ct, real_ct)\n",
    "        vgg_loss = self.vgg(gen_ct, real_ct)\n",
    "        \n",
    "        # Adversarial Loss\n",
    "        adv_loss = -torch.mean(D_fake)\n",
    "        \n",
    "        # Gradient Penalty\n",
    "        gp = self._gradient_penalty(D, real_ct, gen_ct.detach())\n",
    "        \n",
    "        return 100*l1_loss + ms_ssim_loss + 0.1*vgg_loss + 10*(adv_loss + gp)\n",
    "    \n",
    "    def _gradient_penalty(self, D, real, fake):\n",
    "        alpha = torch.rand(real.size(0), 1, 1, 1, device=real.device)\n",
    "        interpolates = (alpha * real + ((1 - alpha) * fake)).requires_grad_(True)\n",
    "        d_interpolates = D(interpolates)\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones_like(d_interpolates),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "def psnr(output, target):\n",
    "    \"\"\"Compute PSNR between [-1,1] normalized tensors\"\"\"\n",
    "    output = (output + 1) / 2  # [-1,1] → [0,1]\n",
    "    target = (target + 1) / 2\n",
    "    mse = torch.mean((output - target) ** 2)\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e218966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorBoardLogger:\n",
    "    def __init__(self, log_dir):\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        \n",
    "    def log_scalar(self, tag, value, step):\n",
    "        self.writer.add_scalar(tag, value, step)\n",
    "        \n",
    "    def log_images(self, tag, images, step):\n",
    "        self.writer.add_images(tag, images, step)\n",
    "        \n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "def visualize_samples(generator, dataloader, device, num_samples=3):\n",
    "    generator.eval()\n",
    "    pet_batch, ct_batch = next(iter(dataloader))\n",
    "    with torch.no_grad():\n",
    "        fake_ct = generator(pet_batch.to(device)).cpu()\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(3, num_samples, i+1)\n",
    "        plt.imshow(pet_batch[i][0], cmap='gray')\n",
    "        plt.title(\"PET Input\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, num_samples, i+1+num_samples)\n",
    "        plt.imshow(fake_ct[i][0], cmap='gray')\n",
    "        plt.title(\"Generated CT\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, num_samples, i+1+2*num_samples)\n",
    "        plt.imshow(ct_batch[i][0], cmap='gray')\n",
    "        plt.title(\"Real CT\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b9f01f",
   "metadata": {},
   "source": [
    "#### 6. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Initialize\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger = TensorBoardLogger(config.tb_log_dir)\n",
    "    os.makedirs(config.model_dir, exist_ok=True)\n",
    "    \n",
    "    # Data\n",
    "    train_ids, val_ids, test_ids = get_patient_splits(config.processed_dir)\n",
    "    train_dataset = QinBreastDataset(config.processed_dir, train_ids)\n",
    "    val_dataset = QinBreastDataset(config.processed_dir, val_ids)\n",
    "    test_dataset = QinBreastDataset(config.processed_dir, test_ids)\n",
    "    \n",
    "    # Model & Optimizers\n",
    "    G = Generator().to(device)\n",
    "    D = MultiScaleDiscriminator().to(device)\n",
    "    opt_G = torch.optim.Adam(G.parameters(), lr=config.lr_g, \n",
    "                            betas=(config.beta1, config.beta2))\n",
    "    opt_D = torch.optim.Adam(D.parameters(), lr=config.lr_d, \n",
    "                            betas=(config.beta1, config.beta2))\n",
    "    criterion = TotalLoss()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        # Training\n",
    "        G.train()\n",
    "        D.train()\n",
    "        train_loss_g = []\n",
    "        train_loss_d = []\n",
    "        \n",
    "        for pet, ct in tqdm(train_loader, desc=f\"Train Epoch {epoch}\"):\n",
    "            pet, ct = pet.to(device), ct.to(device)\n",
    "            \n",
    "            # Discriminator Update\n",
    "            opt_D.zero_grad()\n",
    "            with autocast():\n",
    "                fake_ct = G(pet)\n",
    "                D_real = D(ct)\n",
    "                D_fake = D(fake_ct.detach())\n",
    "                loss_D = criterion(fake_ct, ct, D_real, D_fake, D)\n",
    "            scaler.scale(loss_D).backward()\n",
    "            scaler.step(opt_D)\n",
    "            train_loss_d.append(loss_D.item())\n",
    "            \n",
    "            # Generator Update\n",
    "            opt_G.zero_grad()\n",
    "            with autocast():\n",
    "                fake_ct = G(pet)\n",
    "                D_fake = D(fake_ct)\n",
    "                loss_G = criterion(fake_ct, ct, D_real, D_fake, D)\n",
    "            scaler.scale(loss_G).backward()\n",
    "            scaler.step(opt_G)\n",
    "            scaler.update()\n",
    "            train_loss_g.append(loss_G.item())\n",
    "        \n",
    "        # Logging\n",
    "        avg_train_g = np.mean(train_loss_g)\n",
    "        avg_train_d = np.mean(train_loss_d)\n",
    "        logger.log_scalar('Loss/Train_G', avg_train_g, epoch)\n",
    "        logger.log_scalar('Loss/Train_D', avg_train_d, epoch)\n",
    "        \n",
    "        # Validation\n",
    "        G.eval()\n",
    "        val_loss = []\n",
    "        val_psnr = []\n",
    "        val_ssim = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for pet, ct in tqdm(val_loader, desc=\"Validating\"):\n",
    "                pet, ct = pet.to(device), ct.to(device)\n",
    "                fake_ct = G(pet)\n",
    "                \n",
    "                # Loss\n",
    "                loss = criterion(fake_ct, ct, D(ct), D(fake_ct), D)\n",
    "                val_loss.append(loss.item())\n",
    "                \n",
    "                # Metrics\n",
    "                fake_ct = (fake_ct + 1) / 2\n",
    "                ct_norm = (ct + 1) / 2\n",
    "                val_psnr.append(psnr(fake_ct, ct_norm).cpu().numpy())\n",
    "                val_ssim.append(ms_ssim(fake_ct, ct_norm).cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = np.mean(val_loss)\n",
    "        logger.log_scalar('Loss/Val', avg_val_loss, epoch)\n",
    "        logger.log_scalar('Metrics/Val_PSNR', np.mean(val_psnr), epoch)\n",
    "        logger.log_scalar('Metrics/Val_SSIM', np.mean(val_ssim), epoch)\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improve = 0\n",
    "            torch.save(G.state_dict(), f\"{config.model_dir}/best_G.pth\")\n",
    "            torch.save(D.state_dict(), f\"{config.model_dir}/best_D.pth\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            \n",
    "        if no_improve >= config.patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "            \n",
    "        # Visualization\n",
    "        if epoch % 5 == 0:\n",
    "            visualize_samples(G, val_loader, device)\n",
    "            \n",
    "    # Final Test Evaluation\n",
    "    G.load_state_dict(torch.load(f\"{config.model_dir}/best_G.pth\"))\n",
    "    test_metrics = evaluate(G, test_loader, device)\n",
    "    logger.log_scalar('Metrics/Test_PSNR', test_metrics['psnr'], epoch)\n",
    "    logger.log_scalar('Metrics/Test_SSIM', test_metrics['ssim'], epoch)\n",
    "    \n",
    "    logger.close()\n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0083d84c",
   "metadata": {},
   "source": [
    "####  6. Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    metrics = {\n",
    "        'psnr': [],\n",
    "        'ssim': [],\n",
    "        'mae': [],\n",
    "        'lpips': []\n",
    "    }\n",
    "    \n",
    "    # Initialize LPIPS (Perceptual Metric)\n",
    "    lpips_model = LPIPS(net='vgg').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for pet, ct in tqdm(loader, desc=\"Evaluating\"):\n",
    "            pet, ct = pet.to(device), ct.to(device)\n",
    "            fake_ct = model(pet)\n",
    "            \n",
    "            # Convert to [0,1]\n",
    "            fake_ct_norm = (fake_ct + 1) / 2\n",
    "            ct_norm = (ct + 1) / 2\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics['psnr'].append(psnr(fake_ct_norm, ct_norm).cpu().numpy())\n",
    "            metrics['ssim'].append(ms_ssim(fake_ct_norm, ct_norm).cpu().numpy())\n",
    "            metrics['mae'].append(torch.abs(fake_ct_norm - ct_norm).mean().item())\n",
    "            metrics['lpips'].append(lpips_model(fake_ct_norm, ct_norm).item())\n",
    "    \n",
    "    # Aggregate results\n",
    "    return {k: (np.mean(v), np.std(v)) for k, v in metrics.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e7b02",
   "metadata": {},
   "source": [
    "#### 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b615a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config_path):\n",
    "    config = Config()\n",
    "    config.load(config_path)\n",
    "    \n",
    "    # Run training with loaded config\n",
    "    results = train()\n",
    "    \n",
    "    # Save results\n",
    "    with open(f\"{config.tb_log_dir}/results.yaml\", 'w') as f:\n",
    "        yaml.dump(results, f)\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Example hyperparameter config file\n",
    "hyperparams = \"\"\"\n",
    "batch_size: 16\n",
    "lr_g: 0.0002\n",
    "lr_d: 0.0001\n",
    "lambda_l1: 100\n",
    "lambda_ms_ssim: 1\n",
    "lambda_vgg: 0.1\n",
    "\"\"\"\n",
    "\n",
    "with open(\"hp_config.yaml\", 'w') as f:\n",
    "    f.write(hyperparams)\n",
    "\n",
    "# Run multiple experiments\n",
    "# for hp in hyperparam_grid:\n",
    "#     run_experiment(hp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ebd715",
   "metadata": {},
   "source": [
    "#### 8. Execution & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Start TensorBoard: !tensorboard --logdir=logs/\n",
    "    \n",
    "    # Train with default config\n",
    "    base_metrics = train()\n",
    "    \n",
    "    # Compare with different hyperparams\n",
    "    # tuned_metrics = run_experiment(\"hp_config.yaml\")\n",
    "    \n",
    "    # Visualize comparisons\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(f\"Base Model PSNR: {base_metrics['psnr'][0]:.2f} ± {base_metrics['psnr'][1]:.2f}\")\n",
    "    # print(f\"Tuned Model PSNR: {tuned_metrics['psnr'][0]:.2f} ± {tuned_metrics['psnr'][1]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
